import asyncio
import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple, Union

from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Vertical
from textual.css.query import NoMatches
from textual.widgets import Footer, Header

import globals
from collectors.base import collector_registry
from collectors.top_processes import TopProcessCollector
from utils.log_writer import LogWriter
from utils.memory_cache import MetricCache
from widgets import (
    CurrentTimeWidget,
    DmesgErrorsWidget,
    DockerStatsWidget,
    SystemInfoWidget,
    TopProcessesWidget,
    UptimeWidget,
)

COLLECTION_INTERVAL_SECONDS: int = 2
METRIC_CACHE_TTL_SECONDS: int = 300
LOG_DIR_NAME: str = "logs"


class MonitoringDashboardApp(App[None]):
    CSS_PATH = (
        "dashboard.tcss"  # Assuming dashboard.tcss is in the same directory or accessible path
    )
    TITLE = "ğŸš€ ì‹¤ì‹œê°„ ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ"
    BINDINGS = [
        Binding("q", "quit", "ì¢…ë£Œ"),
        Binding("ctrl+c", "quit", "ì¢…ë£Œ"),
        Binding("d", "toggle_dark", "ë‹¤í¬ ëª¨ë“œ ì „í™˜"),
    ]

    def __init__(self) -> None:
        super().__init__()
        self.metric_cache: MetricCache = MetricCache(ttl_seconds=METRIC_CACHE_TTL_SECONDS)
        self._initialize_collectors_and_logger()
        self.docker_metrics_buffer: Dict[str, Dict[str, Any]] = {}

    def _initialize_collectors_and_logger(self) -> None:
        """ì»¬ë ‰í„°ì™€ ë¡œê±°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        instantiated_collectors = globals.get_instantiated_collectors()
        if not instantiated_collectors:
            collectors_to_init = []
            for collector_cls in collector_registry:
                try:
                    collectors_to_init.append(collector_cls())
                except Exception as e:
                    msg = f"ì»¬ë ‰í„° {collector_cls.__name__} ì¸ìŠ¤í„´ìŠ¤í™” ì‹¤íŒ¨: {e}"
                    # Use self.log if available (after app init), otherwise print
                    if hasattr(self, "log"):
                        self.log.error(msg)
                    else:
                        print(f"ERROR: {msg}")  # Early error
            globals.set_instantiated_collectors(collectors_to_init)

        current_collectors = globals.get_instantiated_collectors()
        if not current_collectors:
            msg = "ì¸ìŠ¤í„´ìŠ¤í™”ëœ ì»¬ë ‰í„° ì—†ìŒ. ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë¶ˆê°€."
            if hasattr(self, "log"):
                self.log.error(msg)
            else:
                print(f"ERROR: {msg}")

        log_writer = globals.get_log_writer_instance()
        if log_writer is None:
            try:
                base_dir_for_logs = Path(__file__).resolve().parent
                main_log_path = base_dir_for_logs / LOG_DIR_NAME
                log_writer = LogWriter(log_dir=main_log_path)
                globals.set_log_writer_instance(log_writer)
                msg = f"LogWriter ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ë¨. ë¡œê·¸ ìœ„ì¹˜: {log_writer.log_dir}"
                if hasattr(self, "log"):
                    self.log.info(msg)
                else:
                    print(f"INFO: {msg}")
            except Exception as e:
                msg = f"LogWriter ì´ˆê¸°í™” ì‹¤íŒ¨: {e}"
                if hasattr(self, "log"):
                    self.log.error(msg)
                else:
                    print(f"ERROR: {msg}")

    def compose(self) -> ComposeResult:
        """ì•±ì˜ ë ˆì´ì•„ì›ƒì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        yield Header(show_clock=False)
        with Container(id="app-grid"):
            with Vertical(id="left-column"):
                yield UptimeWidget(id="uptime")
                yield SystemInfoWidget(id="sys_info")
                yield DmesgErrorsWidget(id="dmesg_errors")
            with Vertical(id="right-column"):
                yield TopProcessesWidget(id="top_procs")
                yield DockerStatsWidget(id="docker_stats")
        yield Footer()
        yield CurrentTimeWidget(id="custom_clock")  # Renamed from CurrentTime to CurrentTimeWidget

    async def on_mount(self) -> None:
        """ì•± ë§ˆìš´íŠ¸ ì‹œ ë¹„ë™ê¸° ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        log_writer = globals.get_log_writer_instance()
        if log_writer:
            try:
                # LogWriter.start might not be async, ensure it's called correctly
                if asyncio.iscoroutinefunction(log_writer.start):
                    await log_writer.start()
                else:
                    log_writer.start()  # Assuming start creates a task
                self.log.info("LogWriter ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘ë¨.")
            except (
                RuntimeError
            ) as e:  # Catch specific error if loop is already running for the task
                self.log.error(
                    f"LogWriter ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}. ì´ë¯¸ ë£¨í”„ê°€ ìˆê±°ë‚˜ ë‹¤ë¥¸ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
            except Exception as e:  # Catch other potential errors
                self.log.error(f"LogWriter ì‹œì‘ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e}")
        else:
            self.log.warning("LogWriterê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. íŒŒì¼ ë¡œê¹…ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

        if not globals.get_instantiated_collectors():
            self.log.error("ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰í„°ê°€ ì—†ì–´ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        self.set_interval(COLLECTION_INTERVAL_SECONDS, self.run_metric_collection_background)
        self.log.info("ëŒ€ì‹œë³´ë“œ ì´ˆê¸°í™” ì™„ë£Œ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘.")

    async def run_metric_collection_background(self) -> None:
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê³  UIë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        current_time_utc = datetime.datetime.now(datetime.timezone.utc)
        total_cpu_sum: float = 0.0
        total_cpu_cores: int = 0
        temp_per_core_cpu: Dict[str, float] = {}
        all_top_processes_data: List[Dict[str, Any]] = []  # Explicitly for TopProcessCollector
        self.docker_metrics_buffer.clear()  # For DockerStatsCollector

        log_writer = globals.get_log_writer_instance()
        instantiated_collectors = globals.get_instantiated_collectors()

        for collector_instance in instantiated_collectors:
            collector_name = collector_instance.__class__.__name__
            try:
                loop = asyncio.get_running_loop()
                # The collected_data can be List[Tuple[str, Any]] or List[Dict[str, Any]]
                collected_data: Union[List[Tuple[str, Any]], List[Dict[str, Any]], List[Any]]
                collected_data = await loop.run_in_executor(None, collector_instance.collect)

                if not collected_data:
                    continue

                # Handle data based on collector type or data structure
                if isinstance(collector_instance, TopProcessCollector):
                    # This data is List[Dict[str, Any]]
                    all_top_processes_data = collected_data  # type: ignore [assignment]
                    if log_writer:
                        for proc_info in all_top_processes_data:  # proc_info is a Dict
                            pid = proc_info.get("pid", "unknown")
                            name = str(proc_info.get("name", "unknown_proc"))
                            # Sanitize name for URI if necessary
                            clean_name = "".join(
                                c if c.isalnum() or c in ("-", "_", ".") else "_" for c in name
                            ).strip("_")
                            if proc_info.get("cpu_percent") is not None:
                                await log_writer.append(
                                    {
                                        "ts": current_time_utc.isoformat(),
                                        "uri": f"top_cpu.{clean_name}.pid_{pid}.cpu_percent",
                                        "value": proc_info["cpu_percent"],
                                        "source": collector_name,
                                    }
                                )
                            if proc_info.get("memory_percent") is not None:
                                await log_writer.append(
                                    {
                                        "ts": current_time_utc.isoformat(),
                                        "uri": f"top_mem.{clean_name}.pid_{pid}.mem_percent",
                                        "value": proc_info["memory_percent"],
                                        "source": collector_name,
                                    }
                                )
                    # Skip to next collector as TopProcessCollector data is handled
                    continue

                # For other collectors, assume List[Tuple[str, Any]]
                # This needs to be safe if collected_data is not a list of tuples
                metrics_tuples = collected_data  # type: ignore [assignment]

                for item in metrics_tuples:  # item is Tuple[str, Any]
                    if not (isinstance(item, tuple) and len(item) == 2):
                        self.log.warning(
                            f"ì»¬ë ‰í„° {collector_name}ì—ì„œ ì˜ëª»ëœ í˜•ì‹ì˜ ë©”íŠ¸ë¦­ ë°ì´í„° ìˆ˜ì‹ : {item}"
                        )
                        continue  # Skip malformed item

                    uri, value = item

                    await self.metric_cache.update(uri, value, current_time_utc)
                    if log_writer:
                        log_entry: Dict[str, Any] = {
                            "ts": current_time_utc.isoformat(),
                            "uri": uri,
                            "value": value,
                            "source": collector_name,
                        }
                        await log_writer.append(log_entry)

                    # Update widgets based on URI
                    self.update_widget_data(uri, value, temp_per_core_cpu)

                    # Aggregate CPU and Docker data
                    if isinstance(value, (int, float)):  # Ensure value is numeric for these calcs
                        if uri.startswith("system.cpu.core"):
                            total_cpu_sum += float(value)
                            total_cpu_cores += 1
                        elif uri.startswith("docker.container."):
                            parts = uri.split(".")
                            if len(parts) > 3:  # e.g., docker.container.NAME.metric_type
                                container_name = parts[2]
                                metric_type = parts[
                                    3
                                ]  # e.g., cpu_percent, mem_percent, mem_usage_mb
                                if container_name not in self.docker_metrics_buffer:
                                    self.docker_metrics_buffer[container_name] = {
                                        "name": container_name
                                    }
                                self.docker_metrics_buffer[container_name][metric_type] = value
            except Exception as e:
                self.log.error(f"ì»¬ë ‰í„° {collector_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # Update SystemInfoWidget with aggregated CPU data
        try:
            sys_info_widget = self.query_one(SystemInfoWidget)
            if total_cpu_cores > 0:
                sys_info_widget.cpu_usage_overall = total_cpu_sum / total_cpu_cores
            else:
                sys_info_widget.cpu_usage_overall = 0.0  # Avoid division by zero
            sys_info_widget.cpu_usage_per_core = temp_per_core_cpu
        except NoMatches:
            self.log.warning(
                "SystemInfoWidgetì„ ì°¾ì„ ìˆ˜ ì—†ì–´ CPU ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            )

        # Update TopProcessesWidget
        if all_top_processes_data:  # Check if data was collected
            try:
                top_procs_widget = self.query_one(TopProcessesWidget)
                top_procs_widget.update_processes(all_top_processes_data)
            except NoMatches:
                self.log.warning("TopProcessesWidgetì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì—…ë°ì´íŠ¸í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # Update DockerStatsWidget
        current_docker_stats_list = list(self.docker_metrics_buffer.values())
        if current_docker_stats_list:  # Check if there's any docker data
            try:
                docker_stats_widget = self.query_one(DockerStatsWidget)
                docker_stats_widget.update_docker_stats(current_docker_stats_list)
            except NoMatches:
                self.log.warning("DockerStatsWidgetì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì—…ë°ì´íŠ¸í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    def update_widget_data(self, uri: str, value: Any, temp_per_core_cpu: Dict[str, float]) -> None:
        """ìˆ˜ì§‘ëœ ë©”íŠ¸ë¦­ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ë‹¹ ìœ„ì ¯ì˜ ë°ì´í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            if uri.startswith("system.cpu.core") and isinstance(value, (int, float)):
                temp_per_core_cpu[uri] = float(value)  # Aggregated later
            elif uri == "system.memory.used_percent" and isinstance(value, (int, float)):
                self.query_one(SystemInfoWidget).mem_usage_percent = float(value)
            elif uri == "system.uptime.description":  # UptimeCollector now returns this
                self.query_one(UptimeWidget).uptime_str = str(value)
            elif uri == "kernel.dmesg.errors":
                if isinstance(value, (int, float)):
                    self.query_one(DmesgErrorsWidget).error_count = int(value)
            # Docker metrics are handled by populating self.docker_metrics_buffer
            # and then updating the DockerStatsWidget in run_metric_collection_background
        except NoMatches:
            # This can happen if a widget is not found, e.g. during app startup/shutdown
            self.log.warning(
                f"'{uri}'ì— ëŒ€í•œ ìœ„ì ¯ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. UIê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        except Exception as e:
            self.log.error(f"ìœ„ì ¯ ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ({uri}: {value}): {e}")

    async def on_shutdown_request(self, _event: Any) -> None:
        """ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ìš”ì²­ ì‹œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        self.log.info("ì• í”Œë¦¬ì¼€ì´ì…˜ ì¢…ë£Œ ìš”ì²­ ìˆ˜ì‹ ...")
        log_writer = globals.get_log_writer_instance()
        if log_writer:
            self.log.info("ë¡œê·¸ í í”ŒëŸ¬ì‹œ ì¤‘...")
            try:
                await log_writer.append(
                    {
                        "ts": datetime.datetime.now(datetime.timezone.utc).isoformat(),
                        "event": "application_shutdown_dashboard",
                        "message": "ëŒ€ì‹œë³´ë“œ ë¡œê¹… ì„œë¹„ìŠ¤ ì¤‘ì§€ ì‹œë„.",
                    }
                )

                # Ensure queue processing is complete
                if hasattr(log_writer, "queue") and hasattr(log_writer.queue, "join"):
                    self.log.info("log_writer.queue.join() ëŒ€ê¸° ì¤‘...")
                    try:
                        # Allow some time for the queue to be processed
                        await asyncio.wait_for(log_writer.queue.join(), timeout=5.0)
                        self.log.info("Log writer queue ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except asyncio.TimeoutError:
                        self.log.warning("log_writer.queue.join() ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼.")
                    except Exception as e_join:
                        self.log.error(f"log_writer.queue.join() ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_join}")

                # Cancel the background task
                if hasattr(log_writer, "task") and log_writer.task and not log_writer.task.done():
                    self.log.info("LogWriter ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì·¨ì†Œ ì¤‘...")
                    log_writer.task.cancel()
                    try:
                        await log_writer.task  # Wait for the task to acknowledge cancellation
                    except asyncio.CancelledError:
                        self.log.info("LogWriter ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    except Exception as e_task_cancel:
                        # Log any other errors that might occur during task cancellation
                        self.log.error(f"ì·¨ì†Œëœ LogWriter ì‘ì—… ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e_task_cancel}")
            except Exception as e:  # Catch-all for other issues during shutdown logic
                self.log.error(f"LogWriter ì¢…ë£Œ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
        self.log.info("ì¢…ë£Œ ì™„ë£Œ.")
